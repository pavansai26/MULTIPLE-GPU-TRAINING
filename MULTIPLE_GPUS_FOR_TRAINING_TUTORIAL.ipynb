{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO/NK73xPVivTLurvQ8sEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/MULTIPLE-GPU-TRAINING/blob/main/MULTIPLE_GPUS_FOR_TRAINING_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Z1eVfjdqqq",
        "outputId": "42f3df1e-f860-405e-b4ae-2d85d716aaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are represented with string identifiers for example:"
      ],
      "metadata": {
        "id": "03BvYqZDf6Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   \"/device:CPU:0\": The CPU of your machine.\n",
        "*   \"/GPU:0\": Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n",
        "\n"
      ],
      "metadata": {
        "id": "_rwqQbqMgANK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a TensorFlow operation has both CPU and GPU implementations, by default, the GPU device is prioritized when the operation is assigned. For example, tf.matmul has both CPU and GPU kernels and on a system with devices CPU:0 and GPU:0, the GPU:0 device is selected to run tf.matmul unless you explicitly request to run it on another device."
      ],
      "metadata": {
        "id": "tb5thvQUg-VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a TensorFlow operation has no corresponding GPU implementation, then the operation falls back to the CPU device."
      ],
      "metadata": {
        "id": "yGxLU87lhE6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Create some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YimJvCgfWLx",
        "outputId": "65ec9e4b-f5e2-4494-8508-377e83d88f43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code will print an indication the MatMul op was executed on GPU:0.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9oqxnkNhxXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with tf.device to create a device."
      ],
      "metadata": {
        "id": "kqZW94BDiIWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Place tensors on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Run on the GPU\n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQntRKGEhow3",
        "outputId": "e2954fa2-ea84-4763-e7e3-58d29630215d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JAePpIqiaac",
        "outputId": "38e56f93-8da6-4dac-e9b7-eba18c84e6ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual devices cannot be modified after being initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process. This is common practice for local development when the GPU is shared with other applications"
      ],
      "metadata": {
        "id": "c8QuD4LCmM8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a single GPU on a multi-GPU system"
      ],
      "metadata": {
        "id": "nzp45Vv1mT9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly"
      ],
      "metadata": {
        "id": "5mpW1Dz-mX0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "try:\n",
        "  # Specify an invalid GPU device\n",
        "  with tf.device('/device:GPU:2'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "    c = tf.matmul(a, b)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQLmHcgGlz8A",
        "outputId": "45917d73-de34-4ac4-c80a-4780178db29c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the device you have specified does not exist, you will get a RuntimeError: .../device:GPU:2 unknown device.\n",
        "\n",
        "If you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn't exist, you can call tf.config.set_soft_device_placement(True)."
      ],
      "metadata": {
        "id": "xaHGOucQmm01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Creates some tensors\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIDVPeWOmc0Q",
        "outputId": "c6282e11-60df-4117-ef66-4331be1420f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using multiple GPUs"
      ],
      "metadata": {
        "id": "ufddjK10tJTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developing for multiple GPUs will allow a model to scale with the additional resources. If developing on a system with a single GPU, you can simulate multiple GPUs with virtual devices. This enables easy testing of multi-GPU setups without requiring additional resources."
      ],
      "metadata": {
        "id": "Az6-5_RGtXHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Create 2 virtual GPUs with 1GB memory each\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
        "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghk_EJzhmtXh",
        "outputId": "87ccd529-30be-42d9-c133-8bb01ac49171"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual devices cannot be modified after being initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-nUXbXItZyM"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}